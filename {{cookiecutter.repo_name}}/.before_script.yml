# https://docs.gitlab.com/ee/ci/yaml/#yaml-anchors-for-script
# could group these commands into logical chunks
# .something: &something
#   - echo 'something'
# job_name:
#   script:
#     - *something
#     - echo 'this is the script'
# but if pyyaml doesn't support that then would be hard to convert to script for Docker build

default:
  cache:
    key: share-cache-across-all-branches-all-jobs
    paths:
    - pki/nssdb/
  before_script:
    - right_after_pull_docker_image=$(date +%s)
    - cat /etc/os-release || echo "cat /etc/os-release failed."
    - lsb_release -a || echo "lsb_release -a failed."
    - hostnamectl || echo "hostnamectl failed."
    - uname -r || echo "uname -r failed."
    - echo $(whoami)
    - echo $USER

    # If we need to apk add openssh-client, then we will need HTTPS_PROXY set first.
    # This potentially leads to a problem if we need SSH to access the ETC_ENVIRONMENT_LOCATION.
    # The ETC_ENVIRONMENT_LOCATION is not generally intended for secret keys like the SSH_PRIVATE_DEPLOY_KEY.
    - if [ -z ${ETC_ENVIRONMENT_LOCATION+ABC} ]; then echo "ETC_ENVIRONMENT_LOCATION is unset, so assuming you do not need environment variables set.";
      else
    # All of this will be skipped unless you set ETC_ENVIRONMENT_LOCATION in GitLab.
    # Note that this will not be skipped if ETC_ENVIRONMENT_LOCATION is set to empty;
    # you could set ETC_ENVIRONMENT_LOCATION to empty for some special behavior, but you're on your own there.
    - if [ -z ${ETC_ENVIRONMENT_LOCATION} ]; then echo "ETC_ENVIRONMENT_LOCATION is set to the empty string; I hope you know why, because I certainly do not."; fi

    # Strictly speaking, this serves the same function as .profile, being run before everything else.
    # You *could* put arbitrary shell commands in the file, but the intended purpose is
    # to save on manual work by allowing you to set only one GitLab variable that points
    # to more variables to set.
    # Special note if the environment file is used to set up a proxy with HTTPS_PROXY...
    # $ETC_ENVIRONMENT_LOCATION must be a location that we can access *before* setting up the proxy variables.
    - echo "ETC_ENVIRONMENT_LOCATION = $ETC_ENVIRONMENT_LOCATION"
    # We do not want the script to hang waiting for a password if the private key is rejected.
    - mkdir --parents ~/.ssh
    - echo "PasswordAuthentication=no" >> ~/.ssh/config

    # The BusyBox version of wget pays attention to http_proxy, but not no_proxy, a dangerous combination.
    # The BusyBox version of wget permits a special option --proxy off to ignore http_proxy.
    # Note the difference from --no-proxy used by GNU wget.
    - ls environment.sh || wget $ETC_ENVIRONMENT_LOCATION --output-document environment.sh --no-clobber || (wget --help && wget --proxy off $ETC_ENVIRONMENT_LOCATION --output-document environment.sh --no-clobber) || curl --verbose $ETC_ENVIRONMENT_LOCATION --output environment.sh || (echo $SSH_PRIVATE_DEPLOY_KEY > SSH.PRIVATE.KEY && scp -i SSH.PRIVATE.KEY $ETC_ENVIRONMENT_LOCATION environment.sh && rm SSH.PRIVATE.KEY)
    # Make sure to clean up that rm SSH.PRIVATE.KEY in case we want to use this script when building a Docker image.
    - cat environment.sh
    # If the environment file wants to hack on our PATH, we usually want to ignore that part.
    - SAVED_PATH=$PATH
    - set -o allexport
    # image gcr.io/kaniko-project/executor:debug (BusyBox v1.31.1) chokes on source environment.sh and also inexplicably chokes on the if-statement or ||
    # - if source environment.sh; then true; else . ./environment.sh; fi
    - . ./environment.sh
    - set +o allexport
    - PATH=$SAVED_PATH
    - fi

    # We'll usually connect to this machine using the loopback IP 127.0.0.1, but just in case some program tries to connect to its public IP address, we want to be sure that won't go through the proxy, if any.
    - hostname -i
    - no_proxy="$(hostname -i),$no_proxy"

    - if [ -z ${SSH_PRIVATE_DEPLOY_KEY+ABC} ]; then echo "SSH_PRIVATE_DEPLOY_KEY is unset, so assuming you do not need SSH set up.";
      else
    # All of this will be skipped unless you set SSH_PRIVATE_DEPLOY_KEY as a variable at https://{{ cookiecutter.repo_hosting_domain }}/{{ cookiecutter.repo_username }}/{{ cookiecutter.repo_name }}/-/settings/ci_cd
    {% raw -%}
    - if [ ${#SSH_PRIVATE_DEPLOY_KEY} -le 5 ]; then echo "SSH_PRIVATE_DEPLOY_KEY looks far too short, something is wrong"; fi
    {%- endraw %}
    - if command -v ssh; then echo "Something that looks like ssh is already installed."; else
    - right_before_install_ssh=$(date +%s)
    - apk add openssh-client || (sed -i -e 's/https/http/' /etc/apk/repositories && apk add openssh-client) || apt-get install --assume-yes openssh-client || (apt-get update && apt-get install --assume-yes openssh-client)  || echo "Failed to install openssh-client; proceeding anyway to see if this image has its own SSH."
    - echo "adding openssh-client took $(( $(date +%s) - right_before_install_ssh)) seconds"
    - fi

    # ssh-agent -s starts the ssh-agent and then outputs shell commands to run.
    - eval $(ssh-agent -s)

    ##
    ## Add the SSH key stored in SSH_PRIVATE_DEPLOY_KEY variable to the agent store.
    ## We're using tr to fix line endings which makes ed25519 keys work
    ## without extra base64 encoding.
    ## We use -d because the version of tr on alpine does not recognize --delete.
    ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556
    ##
    - echo "$SSH_PRIVATE_DEPLOY_KEY" | tr -d '\r' | ssh-add -
    - echo "Added the private SSH deploy key with public fingerprint $(ssh-add -l)"
    - echo "WARNING! If you use this script to build a Docker image (rather than just run tests), make sure to delete the deploy key with ssh-add -D after installing the relevant repos."

    ##
    ## Sometimes we may want to install directly from a git repository.
    ## Using up-to-the-minute updates of dependencies in our own tests alerts
    ## us if something breaks with the latest version of a dependency, even if
    ## that dependency has not made a new release yet.
    ## In order to pip install directly from git repositories,
    ## we need to whitelist the public keys of the git servers.
    ## You may want to add more lines for the domains of any other git servers
    ## you want to install dependencies from (which may or may not include the
    ## server that hosts your own repo).
    ## Similarly, if you want to push to a secondary repo as part of your build
    ## (as how cookiecutter-pylibrary builds examples and
    ## pushes to python-nameless), ssh will need to be allowed to reach that
    ## server.
    ## https://docs.travis-ci.com/user/ssh-known-hosts/
    ## https://discuss.circleci.com/t/add-known-hosts-on-startup-via-config-yml-configuration/12022/2
    ## Unfortunately, there seems to be no way to use ssh-keyscan on a server
    ## that you can only reach through a proxy. Thus, a simple
    ## ssh-keyscan -t rsa github.com gitlab.com >> ~/.ssh/known_hosts
    ## will fail. As a workaround, I just grabbed their public keys now and
    ## included them. These might go stale eventually, I'm not sure.
    ##
    - mkdir --parents ~/.ssh
    - echo "# github.com:22 SSH-2.0-babeld-f345ed5d\n" >> ~/.ssh/known_hosts
    - echo "github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==\n" >> ~/.ssh/known_hosts
    - echo "# gitlab.com:22 SSH-2.0-OpenSSH_7.2p2 Ubuntu-4ubuntu2.8\n" >> ~/.ssh/known_hosts
    - echo "gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\n" >> ~/.ssh/known_hosts
    - fi

    # When we get the environment file, it might have some servers for us to whitelist.
    # Alternatively, maybe there was no ETC_ENVIRONMENT_LOCATION
    # and SERVERS_TO_WHITELIST_FOR_SSH is just manually set as a GitLab variable.
    # If SSH_PRIVATE_DEPLOY_KEY is not set, then we will silently ignore SERVERS_TO_WHITELIST_FOR_SSH,
    # since without a key of some kind we cannot use SSH anyway.
    # This allows us to share around a common ETC_ENVIRONMENT_LOCATION that includes SERVERS_TO_WHITELIST_FOR_SSH,
    # even though only some people actually use SSH for anything.
    - if [ -z ${SERVERS_TO_WHITELIST_FOR_SSH+ABC} ] || [ -z ${SSH_PRIVATE_DEPLOY_KEY+ABC} ]; then echo "SERVERS_TO_WHITELIST_FOR_SSH and SSH_PRIVATE_DEPLOY_KEY are not both set, so assuming you do not need any servers whitelisted for SSH.";
      else
    - echo "SERVERS_TO_WHITELIST_FOR_SSH = $SERVERS_TO_WHITELIST_FOR_SSH"
    - mkdir --parents ~/.ssh
    - ssh-keyscan -t rsa $SERVERS_TO_WHITELIST_FOR_SSH >> ~/.ssh/known_hosts
    - fi

    - if command -v conda; then echo "command finds conda"; else echo "command does not find conda"; fi
    - if [ -d /opt/conda ]; then
    - CONDA_DIR=/opt/conda
    - PATH=$CONDA_DIR/bin:$PATH
    # Now PATH will find conda's "activate" script.
    # Annoyingly, conda activate is not idempotent.
    # We have to avoid activating the env twice, else we will get an error.
    # When a conda env is activated, it sets $CONDA_DEFAULT_ENV to its own name.
    - if [ "$CONDA_DEFAULT_ENV" = "test-env" ]; then echo "This image already has test-env activated.";
      else
    - conda env list
    - fi
    - if [ "$CONDA_DEFAULT_ENV" = "test-env" ] || source activate test-env; then true; else echo "No conda env named test-env was found, so not activating any particular env."; fi
      ; else
      echo "/opt/conda was not found on this container"
      ; fi

    # If PROXY_CA_PEM is provided, we'll merge it with pip's cert store.
    # We unconditionally upgrade pip first, in case pip gets upgraded later in the build.
    - if command -v python; then python -m pip install --upgrade pip; fi

    - if [ -z ${PROXY_CA_PEM+ABC} ]; then echo "PROXY_CA_PEM is unset, so assuming you do not need a merged CA certificate set up.";
      else
    # All of this will be skipped unless you set PROXY_CA_PEM in GitLab.
    # You will usually want to cat your.pem | xclip and paste it in as a File on GitLab.
    # See the KUBE_CA_PEM example at https://docs.gitlab.com/ee/ci/variables/README.html#variable-types
    - right_before_pull_cert=$(date +%s)
    {% raw -%}
    - if [ ${#PROXY_CA_PEM} -ge 1024 ]; then
    {%- endraw %}
    - echo "The PROXY_CA_PEM filename looks far too long, did you set it as a Variable instead of a File?"
    # If it's the full certificate rather than a filename, write it to a file and save the file name.
    - echo "$PROXY_CA_PEM" > tmp-proxy-ca.pem
    # The quotes are very important here; echo $PROXY_CA_PEM will destroy the
    # newlines, and requests will (silently!) fail to parse the certificate,
    # leading to SSLError SSLCertVerificationError 'certificate verify failed self signed certificate in certificate chain (_ssl.c:1076)'
    - PROXY_CA_PEM=tmp-proxy-ca.pem
      ; fi
    - echo "PROXY_CA_PEM found at $(ls $PROXY_CA_PEM)"

    - if command -v wget; then
    - right_before_set_up_wget=$(date +%s)
    # wget --version does not work with BusyBox wget
    # Does BusyBox wget pay attention to .wgetrc?
    - echo "ca_certificate=$(pwd)/$PROXY_CA_PEM" >> $HOME/.wgetrc
    - cat $HOME/.wgetrc
    - echo "Setting up wget took $(( $(date +%s) - right_before_pull_cert)) seconds"
    - fi

    - right_before_install_nss=$(date +%s)
    - if [ -d $HOME/.pki/nssdb ]; then ls -l $HOME/.pki/nssdb/;
      else
    - echo "$HOME/.pki/nssdb not found; looking to create it."
    - if command -v apk; then
    - if [ -d pki/nssdb/ ]; then
    - echo "Found pki/nssdb/ so copying that."
    - else
    - echo "No NSS DB found, but $PROXY_CA_PEM found, so creating an NSS DB."
    - apk add nss-tools || (sed -i -e 's/https/http/' /etc/apk/repositories && apk add nss-tools)
    - mkdir --parents pki/nssdb/
    # create https://www.dogtagpki.org/wiki/NSS_Database#Creating_Database
    # - certutil --help crashes?
    - certutil -N -d pki/nssdb/ --empty-password
    # $HOME/.pki/nssdb seems to be where Chromium looks, at least
    # - certutil -d  -A -t "P,," -n proxycert -i $PROXY_CA_PEM does not seem to matter, the C version is required for chromium at least
    - certutil -d sql:pki/nssdb/ -A -t "C,," -n proxycertasCA -i $PROXY_CA_PEM
    # Ideally we wouldn't apk del this if it was already installed, but apk doesn't seem to provide a way to check that.
    # apk del nss-tools doesn't apk del nss, so this probably isn't a big deal.
    - apk del nss-tools
    - fi
    - ls -l pki/nssdb/
    - mkdir --parents $HOME/.pki/nssdb/
    # Chromium chokes if we symbolic-link the directory
    # ERROR:nss_util.cc(53) Failed to create /root/.pki/nssdb directory.
    # https://chromium.googlesource.com/chromium/src/+/refs/heads/master/crypto/nss_util.cc#46
    # Chromium also chokes if we symbolic-link each individual file
    # - ln -s pki/nssdb/cert9.db $HOME/.pki/nssdb/
    # - ln -s pki/nssdb/key4.db $HOME/.pki/nssdb/
    # - ln -s pki/nssdb/pkcs11.txt $HOME/.pki/nssdb/
    # ERROR nss_util.cc(166) Error initializing NSS with a persistent database (sql:/root/.pki/nssdb) NSS error code -8174
    # So we just copy the entire directory.
    - cp -r pki/nssdb/ $HOME/.pki/
    - echo $HOME/.pki/nssdb
    - ls -l $HOME/.pki/nssdb
    - fi
    - fi
    - echo "adding cert to nss db took $(( $(date +%s) - right_before_install_nss)) seconds"


    - if command -v python; then
    # If some of the links in your documentation require a special PEM to verify,
    # then sphinx -b linkcheck will fail without that PEM.
    # But setting REQUESTS_CA_BUNDLE to that PEM will cause other links to fail,
    # because the runner will only accept that PEM, not the defaults.
    # Therefore you will usually want to bundle all certificates together with
    - python --version
    # cat `python -c "import requests; print(requests.certs.where())"` ~/your.pem > ~/bundled.pem
    # pip uses requests, but not the normal requests.
    # pip uses a vendored version of requests, so that pip will still work if anything goes wrong with your requests installation.
    # We find where that vendored version of requests keeps its certs and merge in the cert from PROXY_CA_PEM.
    # On some systems, we might need to try the import twice, and the first time, it will fail with an AttributeError.
    # Therefore we need a block to suppress the AttributeError, which requires a colon.
    # But that causes parsing of .gitlab-ci.yml to fail with "before_script config should be an array of strings",
    # so we need to wrap the entire line in ''.
    # https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/5481
    # - 'echo -e "import contextlib\nwith contextlib.suppress(AttributeError): import pip._vendor.requests\nfrom pip._vendor.requests.certs import where\nprint(where())" | python'
    # - 'cat `echo -e "import contextlib\nwith contextlib.suppress(AttributeError): import pip._vendor.requests\nfrom pip._vendor.requests.certs import where\nprint(where())" | python` $PROXY_CA_PEM > bundled.pem'
    # Unfortunately, echo -e is not supported on all platforms (onthe official TensorFlow image, in particular),
    # resulting in a mysterious SyntaxError on "-e import contextlib".
    # Thus, we cannot use linebreaks.
    # Still, for the sake of transparency we don't want to call out to an opaque script.
    # We can operate the context manager manually.
    - python -c "import contextlib; contextManager = contextlib.suppress(AttributeError); contextManager.__enter__(); import pip._vendor.requests; contextManager.__exit__(None,None,None); from pip._vendor.requests.certs import where; print(where())"
    - cat $(python -c "import contextlib; contextManager = contextlib.suppress(AttributeError); contextManager.__enter__(); import pip._vendor.requests; contextManager.__exit__(None,None,None); from pip._vendor.requests.certs import where; print(where())") ${PROXY_CA_PEM} > bundled.pem
    - ls bundled.pem
    # In the unlikely event that the image does not have Python available, the above command may silently fail to write bundled.pem.
    # Thus we check python --version above and double-check that bundled.pem exists with ls.
    - export REQUESTS_CA_BUNDLE="${PWD}/bundled.pem"
    # We include the working directory PWD so that REQUESTS_CA_BUNDLE can still be found from another directory.
    # This seems to matter when activating and using a conda environment, for some reason.
    - echo "REQUESTS_CA_BUNDLE found at $(ls $REQUESTS_CA_BUNDLE)"
    - echo "Merging the certificate bundle took $(( $(date +%s) - right_before_pull_cert)) seconds total"
    - fi  # endif command -v python

    - fi  # endif PROXY_CA_PEM is set

    # If you set PYPI_URL, the default behavior is to look first in that index, and second in the default pypi.org.
    # pip does not provide a native option to do this, exactly, but we can hack it together by specifying --extra-index-url https://pypi.org/simple.
    # Note however that if pip finds a higher version number in https://pypi.org, pip will use that.
    # We cannot stop pip from doing that, short of telling pip not to check pypi.org at all (in which case your server would have to be a true PyPI mirror).
    # But so long as PYPI_URL contains a version number greater than or equal to the version number on pypi.org, PYPI_URL will be used.
    # The assumed use-case here is package(s) for which open-source publication chronically lags behind internal publication.
    # When the internal version is ahead, all packages using this template (and with PYPI_URL set) will use the latest version.
    # If it doesn't have internal changes, great! pip will just use the pypi.org version.
    - if [ -z ${PYPI_URL+ABC} ]; then
    - echo "PYPI_URL is set to $PYPI_URL"
    - "if [ ${PYPI_URL: -1} != '/' ]; then"
    - echo "If you try to twine upload without the trailing /, you'll get back an HTTPError 400 Bad Request Repository path must have another '/' after initial '/'."
    - PYPI_URL=$PYPI_URL/
    # https://pip.pypa.io/en/stable/user_guide/#environment-variables
    - PIP_INDEX_URL="${PYPI_URL}simple"
    - echo "PIP_INDEX_URL $PIP_INDEX_URL"
    - PIP_EXTRA_INDEX_URL=https://pypi.org/simple
    - echo "PIP_EXTRA_INDEX_URL $PIP_EXTRA_INDEX_URL"
    - fi

    - fi  # endif PYPI_URL is set

    ##
    ## With all our proxy variables and certificates in place, we should now be
    ## able to install from repositores, and optionally push to repositories.
    ## Optionally, if you will be making any git commits, set the user name and
    ## email.
    ##
    #- git config --global user.email "{{ cookiecutter.email }}"
    #- git config --global user.name "{{ cookiecutter.full_name }}"

    # With --sitepackages, we can save time by installing once
    # for both regular tests and documentation checks.
    # Building the documentation also requires the package to be importable,
    # if using autodoc and its descendants.
    # Note that the installation will be repeated, once for each job.
    # The installation still will not be shared across jobs.
    # This is not ideal, but if installation takes a very long time, then you
    # might want to use a Docker image with most of your dependencies already
    # installed.

    - python3 --version || echo "python3 is not found by that name."
    # If the docs include a Jupyter notebook, we need ipykernel to build the docs (including running doctests).
    # Without ipykernel, attempting to --execute Jupyter notebooks when building the documentation will fail with
    # No such kernel named python3
    - if command -v jupyter; then
    - pip install ipykernel
    - python -m ipykernel install
    - pip install ipywidgets # without this, module 'plotly.graph_objects' has no attribute 'FigureWidget'
    - fi
{% if cookiecutter.docs_require_package == 'yes' %}
    - if command -v python; then
    # We install the package separately so that we can continuously monitor how long installation takes.
    - right_before_pip_install=$(date +%s)
    - python -m pip install .
    - echo "Installing your package took $(( $(date +%s) - right_before_pip_install)) seconds total"
    - fi
{%- endif %}

    - echo "before_script took $(( $(date +%s) - right_after_pull_docker_image)) seconds total"

