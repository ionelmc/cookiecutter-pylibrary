# https://docs.gitlab.com/ee/ci/yaml/#yaml-anchors-for-script
# could group these commands into logical chunks
# .something: &something
#   - echo 'something'
# job_name:
#   script:
#     - *something
#     - echo 'this is the script'
# but if pyyaml doesn't support that then would be hard to convert to script for Docker build

default:
  cache:
    key: share-cache-across-all-branches-all-jobs
    paths:
    - pki/nssdb/
  before_script:
    - right_after_pull_docker_image=$(date +%s)
    - cat /etc/os-release || echo "cat /etc/os-release failed."
    - lsb_release -a || echo "lsb_release -a failed."
    - hostnamectl || echo "hostnamectl failed."
    - uname -r || echo "uname -r failed."
    - echo $(whoami)
    - echo $USER

    # If we need to apk add openssh-client, then we will need HTTPS_PROXY set first.
    # This potentially leads to a problem if we need SSH to access the ETC_ENVIRONMENT_LOCATION.
    # The ETC_ENVIRONMENT_LOCATION is not generally intended for secret keys like the SSH_PRIVATE_DEPLOY_KEY.
    - if [ -z ${ETC_ENVIRONMENT_LOCATION+ABC} ]; then echo "ETC_ENVIRONMENT_LOCATION is unset, so assuming you do not need environment variables set.";
      else
    # All of this will be skipped unless you set ETC_ENVIRONMENT_LOCATION in GitLab.
    # Note that this will not be skipped if ETC_ENVIRONMENT_LOCATION is set to empty;
    # you could set ETC_ENVIRONMENT_LOCATION to empty for some special behavior, but you're on your own there.
    - if [ -z ${ETC_ENVIRONMENT_LOCATION} ]; then echo "ETC_ENVIRONMENT_LOCATION is set to the empty string; I hope you know why, because I certainly do not."; fi

    # Strictly speaking, this serves the same function as .profile, being run before everything else.
    # You *could* put arbitrary shell commands in the file, but the intended purpose is
    # to save on manual work by allowing you to set only one GitLab variable that points
    # to more variables to set.
    # Special note if the environment file is used to set up a proxy with HTTPS_PROXY...
    # $ETC_ENVIRONMENT_LOCATION must be a location that we can access *before* setting up the proxy variables.
    - echo "ETC_ENVIRONMENT_LOCATION = $ETC_ENVIRONMENT_LOCATION"
    # We do not want the script to hang waiting for a password if the private key is rejected.
    - mkdir --parents ~/.ssh
    - echo "PasswordAuthentication=no" >> ~/.ssh/config
    - echo $SSH_PRIVATE_DEPLOY_KEY > SSH.PRIVATE.KEY # If SSH_PRIVATE_DEPLOY_KEY is unset, this will just be empty.
    # The BusyBox version of wget pays attention to http_proxy, but not no_proxy, a dangerous combination.
    # The BusyBox version of wget permits a special option --proxy off to ignore http_proxy.
    # Note the difference from --no-proxy used by GNU wget.
    - wget $ETC_ENVIRONMENT_LOCATION --output-document environment.sh --no-clobber || (wget --help && wget --proxy off $ETC_ENVIRONMENT_LOCATION --output-document environment.sh --no-clobber) || curl --verbose $ETC_ENVIRONMENT_LOCATION --output environment.sh || scp -i SSH.PRIVATE.KEY $ETC_ENVIRONMENT_LOCATION environment.sh
    - rm SSH.PRIVATE.KEY # Make sure to clean up that private key in case we want to use this script when building a Docker image.
    - cat environment.sh
    # If the environment file wants to hack on our PATH, we usually want to ignore that part.
    - SAVED_PATH=$PATH
    - set -o allexport
    # image gcr.io/kaniko-project/executor:debug (BusyBox v1.31.1) chokes on source environment.sh and also inexplicably chokes on the if-statement or ||
    # - if source environment.sh; then true; else . ./environment.sh; fi
    - . ./environment.sh
    - set +o allexport
    - PATH=$SAVED_PATH
    - fi

    - if [ -z ${SSH_PRIVATE_DEPLOY_KEY+ABC} ]; then echo "SSH_PRIVATE_DEPLOY_KEY is unset, so assuming you do not need SSH set up.";
      else
    # All of this will be skipped unless you set SSH_PRIVATE_DEPLOY_KEY as a variable at https://{{ cookiecutter.repo_hosting_domain }}/{{ cookiecutter.repo_username }}/{{ cookiecutter.repo_name }}/-/settings/ci_cd
    {% raw -%}
    - if [ ${#SSH_PRIVATE_DEPLOY_KEY} -le 5 ]; then echo "SSH_PRIVATE_DEPLOY_KEY looks far too short, something is wrong"; fi
    {%- endraw %}
    - apk add openssh-client || apt-get install --assume-yes openssh-client || (apt-get update && apt-get install --assume-yes openssh-client)  || echo "Failed to install openssh-client; proceeding anyway to see if this image has its own SSH."
    - echo "adding openssh-client took $(( $(date +%s) - right_after_pull_docker_image)) seconds"

    # ssh-agent -s starts the ssh-agent and then outputs shell commands to run.
    - eval $(ssh-agent -s)

    ##
    ## Add the SSH key stored in SSH_PRIVATE_DEPLOY_KEY variable to the agent store.
    ## We're using tr to fix line endings which makes ed25519 keys work
    ## without extra base64 encoding.
    ## We use -d because the version of tr on alpine does not recognize --delete.
    ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556
    ##
    - echo "$SSH_PRIVATE_DEPLOY_KEY" | tr -d '\r' | ssh-add -
    - echo "Added the private SSH deploy key with public fingerprint $(ssh-add -l)"
    - echo "WARNING! If you use this script to build a Docker image (rather than just run tests), make sure to delete the deploy key with ssh-add -D after installing the relevant repos."

    ##
    ## Sometimes we may want to install directly from a git repository.
    ## Using up-to-the-minute updates of dependencies in our own tests alerts
    ## us if something breaks with the latest version of a dependency, even if
    ## that dependency has not made a new release yet.
    ## In order to pip install directly from git repositories,
    ## we need to whitelist the public keys of the git servers.
    ## You may want to add more lines for the domains of any other git servers
    ## you want to install dependencies from (which may or may not include the
    ## server that hosts your own repo).
    ## Similarly, if you want to push to a secondary repo as part of your build
    ## (as how cookiecutter-pylibrary builds examples and
    ## pushes to python-nameless), ssh will need to be allowed to reach that
    ## server.
    ## https://docs.travis-ci.com/user/ssh-known-hosts/
    ## https://discuss.circleci.com/t/add-known-hosts-on-startup-via-config-yml-configuration/12022/2
    ## Unfortunately, there seems to be no way to use ssh-keyscan on a server
    ## that you can only reach through a proxy. Thus, a simple
    ## ssh-keyscan -t rsa github.com gitlab.com >> ~/.ssh/known_hosts
    ## will fail. As a workaround, I just grabbed their public keys now and
    ## included them. These might go stale eventually, I'm not sure.
    ##
    - mkdir --parents ~/.ssh
    - echo "# github.com:22 SSH-2.0-babeld-f345ed5d\n" >> ~/.ssh/known_hosts
    - echo "github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==\n" >> ~/.ssh/known_hosts
    - echo "# gitlab.com:22 SSH-2.0-OpenSSH_7.2p2 Ubuntu-4ubuntu2.8\n" >> ~/.ssh/known_hosts
    - echo "gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\n" >> ~/.ssh/known_hosts
    - fi

    # When we get the environment file, it might have some servers for us to whitelist.
    # Alternatively, maybe there was no ETC_ENVIRONMENT_LOCATION
    # and SERVERS_TO_WHITELIST_FOR_SSH is just manually set as a GitLab variable.
    # If SSH_PRIVATE_DEPLOY_KEY is not set, then we will silently ignore SERVERS_TO_WHITELIST_FOR_SSH,
    # since without a key of some kind we cannot use SSH anyway.
    # This allows us to share around a common ETC_ENVIRONMENT_LOCATION that includes SERVERS_TO_WHITELIST_FOR_SSH,
    # even though only some people actually use SSH for anything.
    - if [ -z ${SERVERS_TO_WHITELIST_FOR_SSH+ABC} ] || [ -z ${SSH_PRIVATE_DEPLOY_KEY+ABC} ]; then echo "SERVERS_TO_WHITELIST_FOR_SSH and SSH_PRIVATE_DEPLOY_KEY are not both set, so assuming you do not need any servers whitelisted for SSH.";
      else
    - echo "SERVERS_TO_WHITELIST_FOR_SSH = $SERVERS_TO_WHITELIST_FOR_SSH"
    - mkdir --parents ~/.ssh
    - ssh-keyscan -t rsa $SERVERS_TO_WHITELIST_FOR_SSH >> ~/.ssh/known_hosts
    - fi

    - if command -v conda; then echo "command finds conda"; else echo "command does not find conda"; fi
    - if [ -d /opt/conda ]; then
    - CONDA_DIR=/opt/conda
    - PATH=$CONDA_DIR/bin:$PATH
    # Now PATH will find conda's "activate" script.
    # Annoyingly, conda activate is not idempotent.
    # We have to avoid activating the env twice, else we will get an error.
    # When a conda env is activated, it sets $CONDA_DEFAULT_ENV to its own name.
    - if [ "$CONDA_DEFAULT_ENV" = "test-env" ]; then echo "This image already has test-env activated.";
      else
    - conda env list
    - fi
    - if [ "$CONDA_DEFAULT_ENV" = "test-env" ] || source activate test-env; then true; else echo "No conda env named test-env was found, so not activating any particular env."; fi
      ; else
      echo "/opt/conda was not found on this container"
      ; fi

    # If PROXY_CA_PEM is provided, we'll merge it with pip's cert store.
    # We unconditionally upgrade pip first, in case pip gets upgraded later in the build.
    - if command -v python; then python -m pip install --upgrade pip; fi

    - if [ -z ${PROXY_CA_PEM+ABC} ]; then echo "PROXY_CA_PEM is unset, so assuming you do not need a merged CA certificate set up.";
      else
    # All of this will be skipped unless you set PROXY_CA_PEM in GitLab.
    # You will usually want to cat your.pem | xclip and paste it in as a File on GitLab.
    # See the KUBE_CA_PEM example at https://docs.gitlab.com/ee/ci/variables/README.html#variable-types
    - right_before_pull_cert=$(date +%s)
    {% raw -%}
    - if [ ${#PROXY_CA_PEM} -ge 1024 ]; then
    {%- endraw %}
    - echo "The PROXY_CA_PEM filename looks far too long, did you set it as a Variable instead of a File?"
    # If it's the full certificate rather than a filename, write it to a file and save the file name.
    - echo "$PROXY_CA_PEM" > tmp-proxy-ca.pem
    # The quotes are very important here; echo $PROXY_CA_PEM will destroy the
    # newlines, and requests will (silently!) fail to parse the certificate,
    # leading to SSLError SSLCertVerificationError 'certificate verify failed self signed certificate in certificate chain (_ssl.c:1076)'
    - PROXY_CA_PEM=tmp-proxy-ca.pem
      ; fi
    - echo "PROXY_CA_PEM found at $(ls $PROXY_CA_PEM)"

    - if command -v wget; then
    - right_before_set_up_wget=$(date +%s)
    # wget --version does not work with BusyBox wget
    # Does BusyBox wget pay attention to .wgetrc?
    - echo "ca_certificate=$(pwd)/$PROXY_CA_PEM" >> $HOME/.wgetrc
    - cat $HOME/.wgetrc
    - echo "Setting up wget took $(( $(date +%s) - right_before_pull_cert)) seconds"
    - fi

    - right_before_install_nss=$(date +%s)
    - if [ -d $HOME/.pki/nssdb ]; then echo "$HOME/.pki/nssdb already exists; leaving it alone.";
      else
    - echo "$HOME/.pki/nssdb not found; looking to create it."
    - if [ -d pki/nssdb/ ]; then
    - echo "Found pki/nssdb/ so copying that."
    - else
    - echo "No NSS DB found, but $PROXY_CA_PEM found, so creating an NSS DB."
    - if command -v apk; then
    - apk add nss-tools
    - mkdir --parents pki/nssdb/
    # create https://www.dogtagpki.org/wiki/NSS_Database#Creating_Database
    - certutil --help
    - certutil -N -d pki/nssdb/ --empty-password
    # $HOME/.pki/nssdb seems to be where Chromium looks, at least
    # - certutil -d  -A -t "P,," -n proxycert -i $PROXY_CA_PEM does not seem to matter, the C version is required for chromium at least
    - certutil -d sql:pki/nssdb/ -A -t "C,," -n proxycertasCA -i $PROXY_CA_PEM
    # Ideally we wouldn't apk del this if it was already installed, but apk doesn't seem to provide a way to check that.
    # apk dell nss-tools doesn't apk del nss, so this probably isn't a big deal.
    - apk del nss-tools
    - fi
    - fi
    - ls -l pki/nssdb/
    - mkdir --parents $HOME/.pki/nssdb/
    # Chromium chokes if we symbolic-link the directory
    # ERROR:nss_util.cc(53) Failed to create /root/.pki/nssdb directory.
    # https://chromium.googlesource.com/chromium/src/+/refs/heads/master/crypto/nss_util.cc#46
    # Chromium also chokes if we symbolic-link each individual file
    # - ln -s pki/nssdb/cert9.db $HOME/.pki/nssdb/
    # - ln -s pki/nssdb/key4.db $HOME/.pki/nssdb/
    # - ln -s pki/nssdb/pkcs11.txt $HOME/.pki/nssdb/
    # ERROR nss_util.cc(166) Error initializing NSS with a persistent database (sql:/root/.pki/nssdb) NSS error code -8174
    # So we just copy the entire directory.
    - cp -r pki/nssdb/ $HOME/.pki/
    - echo $HOME/.pki/nssdb
    - ls -l $HOME/.pki/nssdb
    - fi
    - echo "adding cert to nss db took $(( $(date +%s) - right_before_install_nss)) seconds"


    - if command -v python; then
    # If some of the links in your documentation require a special PEM to verify,
    # then sphinx -b linkcheck will fail without that PEM.
    # But setting REQUESTS_CA_BUNDLE to that PEM will cause other links to fail,
    # because the runner will only accept that PEM, not the defaults.
    # Therefore you will usually want to bundle all certificates together with
    - python --version
    # cat `python -c "import requests; print(requests.certs.where())"` ~/your.pem > ~/bundled.pem
    # pip uses requests, but not the normal requests.
    # pip uses a vendored version of requests, so that pip will still work if anything goes wrong with your requests installation.
    # We find where that vendored version of requests keeps its certs and merge in the cert from PROXY_CA_PEM.
    # On some systems, we might need to try the import twice, and the first time, it will fail with an AttributeError.
    # Therefore we need a block to suppress the AttributeError, which requires a colon.
    # But that causes parsing of .gitlab-ci.yml to fail with "before_script config should be an array of strings",
    # so we need to wrap the entire line in ''.
    # https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/5481
    - 'echo -e "import contextlib\nwith contextlib.suppress(AttributeError): import pip._vendor.requests\nfrom pip._vendor.requests.certs import where\nprint(where())" | python'
    - 'cat `echo -e "import contextlib\nwith contextlib.suppress(AttributeError): import pip._vendor.requests\nfrom pip._vendor.requests.certs import where\nprint(where())" | python` $PROXY_CA_PEM > bundled.pem'
    - ls bundled.pem
    # In the unlikely event that the image does not have Python available, the above command may silently fail to write bundled.pem.
    # Thus we check python --version above and double-check that bundled.pem exists with ls.
    - export REQUESTS_CA_BUNDLE="${PWD}/bundled.pem"
    # We include the working directory PWD so that REQUESTS_CA_BUNDLE can still be found from another directory.
    # This seems to matter when activating and using a conda environment, for some reason.
    - echo "REQUESTS_CA_BUNDLE found at $(ls $REQUESTS_CA_BUNDLE)"
    - echo "Merging the certificate bundle took $(( $(date +%s) - right_before_pull_cert)) seconds total"
    - fi

    - fi

    ##
    ## With all our proxy variables and certificates in place, we should now be
    ## able to install from repositores, and optionally push to repositories.
    ## Optionally, if you will be making any git commits, set the user name and
    ## email.
    ##
    #- git config --global user.email "{{ cookiecutter.email }}"
    #- git config --global user.name "{{ cookiecutter.full_name }}"

    # With --sitepackages, we can save time by installing once
    # for both regular tests and documentation checks.
    # Building the documentation also requires the package to be importable,
    # if using autodoc and its descendants.
    # Note that the installation will be repeated, once for each job.
    # The installation still will not be shared across jobs.
    # This is not ideal, but if installation takes a very long time, then you
    # might want to use a Docker image with most of your dependencies already
    # installed.

    - python3 --version || echo "python3 is not found by that name."
    # If the docs include a Jupyter notebook, we need ipykernel to build the docs (including running doctests).
    # Without ipykernel, attempting to --execute Jupyter notebooks when building the documentation will fail with
    # No such kernel named python3
    - if command -v jupyter; then
    - pip install ipykernel
    - python -m ipykernel install
    - pip install ipywidgets # without this, module 'plotly.graph_objects' has no attribute 'FigureWidget'
    - fi
{% if cookiecutter.docs_require_package == 'yes' %}
    - if command -v python; then
    # We install the package separately so that we can continuously monitor how long installation takes.
    - right_before_pip_install=$(date +%s)
    - python -m pip install .
    - echo "Installing your package took $(( $(date +%s) - right_before_pip_install)) seconds total"
    - fi
{%- endif %}

    - echo "before_script took $(( $(date +%s) - right_after_pull_docker_image)) seconds total"

